= Kafka in Kubernetes
include::_attributes.adoc[]
include::https://raw.githubusercontent.com/redhat-developer-demos/rhd-tutorial-common/master/versions.adoc[]

Kubernetes is an open source container orchestration platform that automates many of the manual processes involved in deploying, managing, and scaling containerized applications.

Kubernetes is an ideal platform for hosting cloud-native applications that require rapid scaling, like real-time data streaming through Apache Kafka.

But deploying a Kafka cluster with - or in newer versions without ZooKeeper - and more than one broker, is not an easy task. It is particularly challenging to get the following three aspects right:

* networking
* storage
* security

Instead of trying to manually configure everything from scratch which is both, cumbersome and error-prone, we should delegate this to Kubernetes operators. To do so, we can use https://strimzi.io/[Strimzi] which offers a powerful and convenient way to deploy different Kafka cluster architectures to Kubernetes.

[#kubernetes]
== Kubernetes

For this part of the tutorial you need a Kubernetes cluster running.

[#install-minikube]
=== Install Minikube

include::https://raw.githubusercontent.com/redhat-developer-demos/rhd-tutorial-common/master/install-minikube.adoc[]

[#start-kubernetes]
=== Start Kubernetes

The following section shows how to start Kubernetes with required configurations:

:profile: kafka
include::https://raw.githubusercontent.com/redhat-developer-demos/rhd-tutorial-common/master/kubernetes-setup.adoc[]

[#strimzi]
== Strimzi

Strimzi provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations.

Some of the features of Strimzi are:

Secure by Default:: TLS and SCRAM-SHA supported. Automated Certificate Management.
Simple yet Configurable:: NodePort, Load balancer, and Ingress options. Rack awareness for HA. Use dedicated nodes for Kafka.
Kubernetes-Native Experience:: `kubectl get kafka`. Operator Based. Manage Kafka using gitops.

[#installing-crds]
=== Installation of Strimzi Operator

Strimzi uses Kubernetes operators to manage the Kafka cluster.
To install the Strimzi operator you need to run the following command:


[tabs]
====
Minikube::
+
--
[.console-input]
[source, bash-shell]
----
kubectl apply -f 'https://strimzi.io/install/latest?namespace=default' -n default 
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-broker-delegation created
serviceaccount/strimzi-cluster-operator created
customresourcedefinition.apiextensions.k8s.io/kafkatopics.kafka.strimzi.io created
clusterrole.rbac.authorization.k8s.io/strimzi-entity-operator created
clusterrole.rbac.authorization.k8s.io/strimzi-kafka-broker created
rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-watched created
rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation created
rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-leader-election created
clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-global created
clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-leader-election created
clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-namespaced created
customresourcedefinition.apiextensions.k8s.io/kafkas.kafka.strimzi.io created
deployment.apps/strimzi-cluster-operator created
customresourcedefinition.apiextensions.k8s.io/kafkaconnectors.kafka.strimzi.io created
customresourcedefinition.apiextensions.k8s.io/strimzipodsets.core.strimzi.io created
clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created
customresourcedefinition.apiextensions.k8s.io/kafkaconnects.kafka.strimzi.io created
customresourcedefinition.apiextensions.k8s.io/kafkarebalances.kafka.strimzi.io created
customresourcedefinition.apiextensions.k8s.io/kafkamirrormaker2s.kafka.strimzi.io created
clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-client-delegation created
customresourcedefinition.apiextensions.k8s.io/kafkanodepools.kafka.strimzi.io created
customresourcedefinition.apiextensions.k8s.io/kafkausers.kafka.strimzi.io created
rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created
clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-watched created
clusterrole.rbac.authorization.k8s.io/strimzi-kafka-client created
customresourcedefinition.apiextensions.k8s.io/kafkabridges.kafka.strimzi.io created
configmap/strimzi-cluster-operator created
customresourcedefinition.apiextensions.k8s.io/kafkamirrormakers.kafka.strimzi.io created
----

[.console-input]
[source, bash-shell]
----
kubectl get pods
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
NAME                                        READY   STATUS              RESTARTS   AGE
strimzi-cluster-operator-79fb75cb8d-7k9rj   0/1     ContainerCreating   0          27s
----

After a while the pod becomes ready and is running fine.

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
NAME                                        READY   STATUS    RESTARTS   AGE
strimzi-cluster-operator-79fb75cb8d-7k9rj   1/1     Running   0          97s
----
--
OpenShift::
+
--
To install Kafka in OpenShift, you can go to menu:Operators[OperatorHub] and search for `Kafka`.
Then select the `Red Hat Integration - AMQ Streams` and install it.

image::amqstreams.png[]
--
====

Check that everything has been installed by running the following commands:

[.console-input]
[source, bash-shell]
----
kubectl get crds | grep kafka
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
NAME                                  CREATED AT
kafkabridges.kafka.strimzi.io                 2024-05-14T07:06:10Z
kafkaconnectors.kafka.strimzi.io              2024-05-14T07:06:10Z
kafkaconnects.kafka.strimzi.io                2024-05-14T07:06:10Z
kafkamirrormaker2s.kafka.strimzi.io           2024-05-14T07:06:10Z
kafkamirrormakers.kafka.strimzi.io            2024-05-14T07:06:10Z
kafkanodepools.kafka.strimzi.io               2024-05-14T07:06:10Z
kafkarebalances.kafka.strimzi.io              2024-05-14T07:06:10Z
kafkas.kafka.strimzi.io                       2024-05-14T07:06:10Z
kafkatopics.kafka.strimzi.io                  2024-05-14T07:06:10Z
kafkausers.kafka.strimzi.io                   2024-05-14T07:06:10Z
----

[#deploy-kafka]
=== Deploying Kafka Cluster

To deploy the cluster, you need to create a Kafka resource file using the Kafka Custom Resource Definition (_CRD_).

In this case, you are going to deploy a Kafka cluster with 3 instances and ephemeral storage.

The file you are going to apply is shown below:

[.console-input]
[source, bash-shell]
----
include::https://strimzi.io/examples/latest/kafka/kafka-ephemeral.yaml[]
----

Notice how simple it is to deploy a Kafka cluster in Kubernetes, you just need a few YAML lines.
Run the following command to deploy Kafka:

[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
kubectl apply -f https://strimzi.io/examples/latest/kafka/kafka-ephemeral.yaml -n default
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
kafka.kafka.strimzi.io/my-cluster created
----

To wait until Kafka cluster is up and running, you can run the following command:

[.console-input]
[source, bash-shell]
----
kubectl wait kafka/my-cluster --for=condition=Ready --timeout=300s -n default
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
kafka.kafka.strimzi.io/my-cluster condition met
----

After that, you should have Kafka pods and services up and running:

[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
kubectl get pods
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
NAME                                          READY   STATUS    RESTARTS   AGE
my-cluster-entity-operator-5f6c66f65c-k2vww   2/2     Running   0          29s
my-cluster-kafka-0                            1/1     Running   0          52s
my-cluster-kafka-1                            1/1     Running   0          52s
my-cluster-kafka-2                            1/1     Running   0          52s
my-cluster-zookeeper-0                        1/1     Running   0          2m53s
my-cluster-zookeeper-1                        1/1     Running   0          2m53s
my-cluster-zookeeper-2                        1/1     Running   0          2m53s
strimzi-cluster-operator-79fb75cb8d-7k9rj     1/1     Running   0          6m12s
----

And services:

[.console-input]
[source, bash-shell]
----
kubectl get services
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                        AGE
kubernetes                    ClusterIP   10.96.0.1       <none>        443/TCP                                        12h
my-cluster-kafka-bootstrap    ClusterIP   10.96.137.128   <none>        9091/TCP,9092/TCP,9093/TCP                     72s
my-cluster-kafka-brokers      ClusterIP   None            <none>        9090/TCP,9091/TCP,8443/TCP,9092/TCP,9093/TCP   72s
my-cluster-zookeeper-client   ClusterIP   10.96.88.171    <none>        2181/TCP                                       3m13s
my-cluster-zookeeper-nodes    ClusterIP   None            <none>        2181/TCP,2888/TCP,3888/TCP                     3m13s
----

[#deploy-service-strimzi]
== Deploying Services

Let's deploy the example done at xref:04-java-consumer-producer.adoc##developing-consumers-producers-java[Developing Consumers and Producers in Java], but in Kubernetes instead of Docker.
This time, the Quarkus versions of the `song-app` and `song-indexer-app` are deployed.

[#kubernetes-song-app]
=== Deploy Song Service

To deploy the `song-app` service go to its folder and build the application

[.console-input]
[source, bash-shell]
----
cd $TUTORIAL_HOME/apps/song-app/quarkus/song-app
./mvnw clean package -DskipTests
----

After a successful build use `kubectl` to deploy it:

[.console-input]
[source, bash-shell]
----
kubectl apply -f target/kubernetes/kubernetes.yml
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
service/song-app created
deployment.apps/song-app created
----

Verify that the service has been deployed correctly:

[.console-input]
[source, bash-shell]
----
kubectl get pods
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
NAME                                          READY   STATUS    RESTARTS   AGE
my-cluster-entity-operator-5f6c66f65c-k2vww   2/2     Running   0          12m
my-cluster-kafka-0                            1/1     Running   0          13m
my-cluster-kafka-1                            1/1     Running   0          13m
my-cluster-kafka-2                            1/1     Running   0          13m
my-cluster-zookeeper-0                        1/1     Running   0          15m
my-cluster-zookeeper-1                        1/1     Running   0          15m
my-cluster-zookeeper-2                        1/1     Running   0          15m
song-app-566b5f7bf5-pdl4b                     1/1     Running   0          2m12s
strimzi-cluster-operator-79fb75cb8d-7k9rj     1/1     Running   0          18m
----

[#kubernetes-song-indexer-app]
=== Deploy Song Indexer Service

To deploy the `song-indexer-app` service go to its folder and build the application

[.console-input]
[source, bash-shell]
----
cd $TUTORIAL_HOME/apps/song-indexer-app/quarkus/song-indexer-app
./mvnw clean package -DskipTests
----

And use `kubectl` to deploy the it:

[.console-input]
[source, bash-shell]
----
kubectl apply -f target/kubernetes/kubernetes.yml
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
service/song-indexer-app created
deployment.apps/song-indexer-app created
----

Verify that the service has been deployed correctly:

[.console-input]
[source, bash-shell]
----
kubectl get pods
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
NAME                                          READY   STATUS    RESTARTS   AGE
my-cluster-entity-operator-5f6c66f65c-k2vww   2/2     Running   0          15m
my-cluster-kafka-0                            1/1     Running   0          15m
my-cluster-kafka-1                            1/1     Running   0          15m
my-cluster-kafka-2                            1/1     Running   0          15m
my-cluster-zookeeper-0                        1/1     Running   0          17m
my-cluster-zookeeper-1                        1/1     Running   0          17m
my-cluster-zookeeper-2                        1/1     Running   0          17m
song-app-566b5f7bf5-pdl4b                     1/1     Running   0          4m29s
song-indexer-app-578bfb4d74-6j8sc             1/1     Running   0          26s
strimzi-cluster-operator-79fb75cb8d-7k9rj     1/1     Running   0          20m
----

[#kubernetes-testing]
=== Test It

Let's test that it works as expected.

The first thing we do is create tunnels to the corresponding services of the two applications. This can be easily done with the `minikube service` command.

In a separate terminal window run the following:

[.console-input]
[source, bash-shell]
----
minikube service song-app
----

Your console output should look similar (minikube IP and ports in use may vary of course). In case a browser window opens automatically you can close it because it's not needed in our case.

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
|-----------|---------------------------------|-------------|---------------------------|
| NAMESPACE |              NAME               | TARGET PORT |            URL            |
|-----------|---------------------------------|-------------|---------------------------|
| default   | song-app                        | http/80     | http://192.168.49.2:30588 |
|-----------|---------------------------------|-------------|---------------------------|
🏃  Starting tunnel for service kafka-tutorial-song-app-quarkus.
|-----------|---------------------------------|-------------|------------------------|
| NAMESPACE |              NAME               | TARGET PORT |          URL           |
|-----------|---------------------------------|-------------|------------------------|
| default   | song-app                        |             | http://127.0.0.1:65000 |
|-----------|---------------------------------|-------------|------------------------|
🎉  Opening service default/song-app in default browser...
----

The important part is shown in the lower right box, namely use `127.0.0.1:65000` as host and port for any HTTP commands against the `song-app`.

Again, in a separate terminal window run the following:

[.console-input]
[source, bash-shell]
----
minikube service song-indexer-app
----

Your console output should look similar (minikube IP and ports in use may vary of course). In case a browser window opens automatically you can close it because it's not needed in our case.

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
|-----------|-----------------------------------------|-------------|---------------------------|
| NAMESPACE |                  NAME                   | TARGET PORT |            URL            |
|-----------|-----------------------------------------|-------------|---------------------------|
| default   | song-indexer-app                        | http/80     | http://192.168.49.2:32548 |
|-----------|-----------------------------------------|-------------|---------------------------|
🏃  Starting tunnel for service song-indexer-app
|-----------|-----------------------------------------|-------------|------------------------|
| NAMESPACE |                  NAME                   | TARGET PORT |          URL           |
|-----------|-----------------------------------------|-------------|------------------------|
| default   | song-indexer-app                        |             | http://127.0.0.1:64938 |
|-----------|-----------------------------------------|-------------|------------------------|
🎉  Opening service default/song-indexer-app in default browser...
----

The important part is shown in the lower right box, namely use `127.0.0.1:64938` as host and port for any HTTP commands against the `song-indexer-app`.

With the above, you successfully created tunnels to the two services and you are now ready to communicate with the applications from you host machine. 

Open another 2 terminal windows, one for populating songs, and another one to get the SSE stream from the indexer service.

In the terminal 1 run the following command against the `song-indexer-app`:

[.console-input]
[source, bash-shell]
----
http GET 127.0.0.1:64938/events --stream --timeout=600
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
HTTP/1.1 200 OK
Content-Type: text/event-stream
transfer-encoding: chunked
----

In the terminal 2 run the following command against the `song-app`:

[.console-input]
[source, bash-shell]
----
http POST 127.0.0.1:65000/songs id=107 name=Portals author='Alan Silvestri'
----

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
HTTP/1.1 201 Created
content-length: 0
----

Inspect the output of terminal 1, to check that the published song data has been processed by the `song-indexer-app`:

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----

id:7d92d0ed-ab04-4182-96b0-0bc4f97156c5
data:Song 107 processed

----

[#kubernetes-cleanup]
== Clean Up

To clean up the whole namespace run:

[.console-input]
[source, bash-shell]
----
kubectl delete all --all
----
