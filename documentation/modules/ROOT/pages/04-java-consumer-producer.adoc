= Developing Consumers and Producers in Java
include::_attributes.adoc[]

In this example, we are going to develop two simple services which will produce data to and consume data from a Kafka topic named `songs`.

TIP: By default, it is not necessary to create the Kafka topic manually, Kafka creates it automatically with default parameters.

The first service acts as a Kafka producer to write song related information (e.g. _id_, _author_, _name_) into the `songs` topic. Then there is a second service acting as a Kafka consumer which reads this data from the `songs` topic. Usually this consumer would perform some "real business logic" such as taking the songs data and process it somehow, e.g. adding songs in a graph database like Neo4J, or build a fulltext search index based on Elastic. However, for the sake of simplicity, our service is just printing the consumed data to the console and exposes every consumed Kafka record over a server-sent event (SSE) stream for connected HTTP clients.

include::partial$kafka-up-dc.adoc[]

It's best to have 4 terminal windows open for running this example, 2 terminals for the producer and consumer services, and another 2 terminals for sending and receiving HTTP requests/responses.

image::terminals-song.png[]

[#producer-java]
== Producing messages with Java

The producer code is at {github-repo}/{apps-folder}/song-app[Song App, window=_blank].

[#deploying-producer]
=== Deploying Producer

In this case, the Spring Boot service is deployed to produce songs.
You've got different options, using either pre-built container images run with Podman / Docker or building the application and image on your local machine from the sources.
In terminal 1, run one of these options:

[tabs]
====
[Optional] Local Build & Run Podman::
+
--
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
cd $TUTORIAL_HOME/apps/song-app/springboot/song-app
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
./mvnw clean package -DskipTests
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
podman build -t quay.io/rhdevelopers/kafka-tutorial-song-app-springboot:latest .
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
podman run --rm -it -p 8080:8080 --network=kafka-tutorial quay.io/rhdevelopers/kafka-tutorial-song-app-springboot:latest
----
--
[Optional] Local Build & Run Docker::
+
--
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
cd $TUTORIAL_HOME/apps/song-app/springboot/song-app
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
./mvnw clean package -DskipTests
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
docker build -t quay.io/rhdevelopers/kafka-tutorial-song-app-springboot:latest .
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
docker run --rm -it -p 8080:8080 --network=kafka-tutorial quay.io/rhdevelopers/kafka-tutorial-song-app-springboot:latest
----
--
====

[tabs]
====
Pre-built image with Podman::
+
--
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
podman run --rm -it -p 8080:8080 --network=kafka-tutorial quay.io/rhdevelopers/kafka-tutorial-song-app-springboot:v22.12-xa
----
--
Pre-built image with Docker::
+
--
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
docker run --rm -it -p 8080:8080 --network=kafka-tutorial quay.io/rhdevelopers/kafka-tutorial-song-app-springboot:v22.12-xa
----
--
====

Console output when running the containerized application looks as follows:

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
...
2024-05-13T14:57:49.489Z  INFO 1 --- [           main] org.acme.song.app.SongApplication        : Starting SongApplication v0.0.1-SNAPSHOT using Java 17.0.6 with PID 1 (/deployments/song-app.jar started by jboss in /deployments)
2024-05-13T14:57:49.491Z  INFO 1 --- [           main] org.acme.song.app.SongApplication        : The following 1 profile is active: "prod"
2024-05-13T14:57:49.955Z  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2024-05-13T14:57:49.960Z  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2024-05-13T14:57:49.960Z  INFO 1 --- [           main] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.1]
2024-05-13T14:57:49.993Z  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2024-05-13T14:57:49.994Z  INFO 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 475 ms
2024-05-13T14:57:50.222Z  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
2024-05-13T14:57:50.231Z  INFO 1 --- [           main] org.acme.song.app.SongApplication        : Started SongApplication in 0.931 seconds (process running for 1.148)
----

[#consumer-java]
== Consuming messages with Java

The consumer code is at {github-repo}/{apps-folder}/song-indexer-app[Song Indexer App, window=_blank].

[#deploying-consumer]
=== Deploying Consumer

In this case, the Spring Boot service is deployed to consume songs.
You've got different options, using either pre-built container images run with Podman / Docker or building the application and image on your local machine from the sources.
In terminal 2, run one of these options:

[tabs]
====
[Optional] Local Build & Run Podman::
+
--
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
cd $TUTORIAL_HOME/apps/song-indexer-app/springboot/song-indexer-app
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
./mvnw clean package -DskipTests
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
podman build -t quay.io/rhdevelopers/kafka-tutorial-song-indexer-app-springboot:latest .
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
podman run --rm -it -p 9090:8080 --network=kafka-tutorial quay.io/rhdevelopers/kafka-tutorial-song-indexer-app-springboot:latest
----
--
[Optional] Local Build & Run Docker::
+
--
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
cd $TUTORIAL_HOME/apps/song-app/springboot/song-app
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
./mvnw clean package -DskipTests
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
docker build -t quay.io/rhdevelopers/kafka-tutorial-song-indexer-app-springboot:latest .
----
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
docker run --rm -it -p 9090:8080 --network=kafka-tutorial quay.io/rhdevelopers/kafka-tutorial-song-indexer-app-springboot:latest
----
--
====

[tabs]
====
Pre-built image with Podman::
+
--
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
podman run --rm -it -p 9090:8080 --network=kafka-tutorial quay.io/rhdevelopers/kafka-tutorial-song-indexer-app-springboot:v22.12-xa
----
--
Pre-built image with Docker::
+
--
[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
docker run --rm -it -p 9090:8080 --network=kafka-tutorial quay.io/rhdevelopers/kafka-tutorial-song-indexer-app-springboot:v22.12-xa
----
--
====


[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
...
2024-05-13T15:23:11.572Z  INFO 1 --- [           main] o.a.s.i.app.SongIndexerApplication       : Started SongIndexerApplication in 1.215 seconds (process running for 1.456)
2024-05-13T15:23:11.722Z  INFO 1 --- [ntainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-G1-1, groupId=G1] Resetting the last seen epoch of partition songs-0 to 0 since the associated topicId changed from null to RV2jwIqHTqCXddmgtb1stw
2024-05-13T15:23:11.724Z  INFO 1 --- [ntainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-G1-1, groupId=G1] Cluster ID: DIAO2SH9SdGuqBkcKYXtiw
2024-05-13T15:23:11.725Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
2024-05-13T15:23:11.726Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] (Re-)joining group
2024-05-13T15:23:11.737Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] Request joining group due to: need to re-join with the given member-id: consumer-G1-1-802608ee-06dd-4671-9cc6-1449f30473fa
2024-05-13T15:23:11.738Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2024-05-13T15:23:11.738Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] (Re-)joining group
2024-05-13T15:23:14.743Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] Successfully joined group with generation Generation{generationId=3, memberId='consumer-G1-1-802608ee-06dd-4671-9cc6-1449f30473fa', protocol='range'}
2024-05-13T15:23:14.746Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] Finished assignment for group at generation 3: {consumer-G1-1-802608ee-06dd-4671-9cc6-1449f30473fa=Assignment(partitions=[songs-0])}
2024-05-13T15:23:14.751Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] Successfully synced group in generation Generation{generationId=3, memberId='consumer-G1-1-802608ee-06dd-4671-9cc6-1449f30473fa', protocol='range'}
2024-05-13T15:23:14.752Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] Notifying assignor about the new Assignment(partitions=[songs-0])
2024-05-13T15:23:14.754Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] Adding newly assigned partitions: songs-0
2024-05-13T15:23:14.761Z  INFO 1 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-G1-1, groupId=G1] Setting offset for partition songs-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}
2024-05-13T15:23:14.762Z  INFO 1 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : G1: partitions assigned: [songs-0]
----

[#providing-song]
=== Providing Songs

The song indexer service consume songs that have been produced to the `songs` topic. Besides printing the song to the console it exposes every consumed record as sever-sent event (SSE) over HTTP.
In the terminal 3, run the following command to start listening for sever-sent events over the HTTP connection:

[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
http GET localhost:9090/events --stream --timeout=600
----

The song service can produce songs into the `songs` the topic. For that, it exposes an HTTP POST endpoint which allows us to publish songs.
In the terminal 4, run the following command to publish a song:

[.console-input]
[source, bash-shell,subs="+macros,+attributes"]
----
http POST localhost:8080/songs id=1000 name='Portals' author='Alan Silvestri'
----

[.console-ouput]
[source, bash-shell,subs="+macros,+attributes"]
----
HTTP/1.1 201
Connection: keep-alive
Content-Length: 0
Date: Mon, 13 May 2024 15:26:57 GMT
Keep-Alive: timeout=60
----

To verify that the song has been processed, check terminal 2 (`song indexer service`) for the last couple of log lines:

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
2024-05-13T15:23:14.762Z  INFO 1 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : G1: partitions assigned: [songs-0]
Song[id=1000, name=Portals, author=Alan Silvestri, op=ADD] indexed.
2024-05-13T15:26:57.899Z  INFO 1 --- [ntainer#0-0-C-1] reactor.Flux.SinkManyBestEffort.1        : onNext(ServerSentEvent [id = 'ee19132e-ae48-4c2b-8df9-e42939d19039', event='null', retry=null, comment='null', data=Song 1000 processed])
----

And in the terminal 4, you should see that the sever-sent event has been streamed successfully:

[.console-output]
[source, bash-shell,subs="+macros,+attributes"]
----
HTTP/1.1 200 OK
Content-Type: text/event-stream;charset=UTF-8
Vary: Origin
Vary: Access-Control-Request-Method
Vary: Access-Control-Request-Headers
transfer-encoding: chunked

id:ee19132e-ae48-4c2b-8df9-e42939d19039
data:Song 1000 processed
----

[#java-cleanup]
== Clean Up

Stop the two application containers that are running in the terminal 1 and 2 by typing kbd:[Ctrl + C].

include::partial$kafka-restart.adoc[]